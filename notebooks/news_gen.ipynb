{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Masking (MADE style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml/lpagec/anaconda2/envs/pytorch_py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ml/lpagec/anaconda2/envs/pytorch_py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext import data, datasets\n",
    "from collections import OrderedDict as OD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformer import * \n",
    "from utils       import * \n",
    "from custom_ds   import CustomDataset\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)  \n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "VERBOSE = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "def load_data(train_file='train.txt', valid_file='valid.txt', test_file='test.txt', path=None, **kwargs):\n",
    "    if path is not None:\n",
    "        train_file, valid_file, test_file = [os.path.join(path, ext) for ext in [train_file, valid_file, test_file]]\n",
    "\n",
    "    # create required field for language modeling\n",
    "    input_field = data.Field(lower=True, batch_first=True)\n",
    "    fields = [(\"text\", input_field)]\n",
    "\n",
    "    train_set, valid_set, test_set  = CustomDataset.splits(fields, train_file, valid_file, test_file)\n",
    "    input_field.build_vocab(train_set)\n",
    "\n",
    "    train_loader, val_loader, test_loader = data.Iterator.splits(\n",
    "      (train_set, valid_set, test_set),\n",
    "      sort_key=lambda x : len(x.text),\n",
    "      batch_sizes=(256, 512, 512),\n",
    "      **kwargs)\n",
    "\n",
    "    return input_field, train_loader, val_loader, test_loader\n",
    "\n",
    "input_field, train_iter, val_iter, test_iter = load_data(path='data/news', device=0, repeat=False)\n",
    "iterators = {'train': train_iter, 'valid': val_iter, 'test': test_iter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params 11706517\n"
     ]
    }
   ],
   "source": [
    "# create model and ship to GPU\n",
    "gen  = make_model(len(input_field.vocab.itos), N=2, h=4).cuda()\n",
    "# print(gen)\n",
    "print('number of params', sum([np.prod([int(y) for y in x.shape]) for x in gen.parameters()]))\n",
    "\n",
    "# build optimizer\n",
    "optimizer_gen = torch.optim.Adam(gen.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over the whole dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_epoch(epoch_no, split, mask_type='left to right'):\n",
    "    loader = iterators[split]\n",
    "\n",
    "    # create logging containers\n",
    "    logs = OD()\n",
    "    for name in ['nll', 'ppl']:\n",
    "        logs[name] = []\n",
    "\n",
    "    gen.train() if split == 'train' else gen.eval()\n",
    "\n",
    "    # Training loop\n",
    "    for i, minibatch in enumerate(loader):\n",
    "        \n",
    "        text = minibatch.text.cuda()\n",
    "        input, target = text[:, :-1], text[:, 1:]\n",
    "        \n",
    "        bs, seq_len = input.size()\n",
    "        \n",
    "        if mask_type == 'left to right':\n",
    "            masks = make_std_mask(target, 0)\n",
    "        elif mask_type == 'random':\n",
    "            masks = torch.from_numpy(build_ar_masks([seq_len] * bs)).long().cuda()\n",
    "        else:\n",
    "            raise ValueError('%s is an invalid mask type' % mask_type)\n",
    "\n",
    "        logits = gen(input, masks)\n",
    "        recon_loss = F.cross_entropy(logits.view(bs * seq_len, -1), target.flatten())\n",
    "\n",
    "        if gen.training:\n",
    "             optimizer_gen.zero_grad()\n",
    "             recon_loss.backward()\n",
    "             params = optimizer_gen.param_groups[0]['params']\n",
    "             torch.nn.utils.clip_grad_norm_(params, 10, norm_type=2)\n",
    "             optimizer_gen.step()\n",
    "         \n",
    "        logs['nll']  += [recon_loss.data]\n",
    "        logs['ppl']  += [recon_loss.exp().data]\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot perplexity graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(train_ppl, valid_ppl, title):\n",
    "    train, valid = [], []\n",
    "    for tt, vv in zip(train_ppl, valid_ppl):\n",
    "        train += [torch.stack(tt).mean().item()]\n",
    "        valid += [torch.stack(vv).mean().item()]\n",
    "        \n",
    "    plt.scatter(np.arange(len(train)), train, label='train ppl')\n",
    "    plt.scatter(np.arange(len(valid)), valid, label='valid ppl')\n",
    "    plt.legend()\n",
    "    plt.hlines(min(valid), 0, len(valid), linestyles='dashed')\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlim(-0.1, len(valid))\n",
    "    plt.yticks([min(valid)] + [x for x in np.linspace(0, max(train + valid), 5)][1:])\n",
    "    plt.title(title)\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp1 : Baseline Model (left to right masking for training and eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ppl, valid_ppl, writes = [], [], 0\n",
    "\n",
    "for epoch in range(0):\n",
    "    train_log  = full_epoch(epoch, 'train', mask_type = 'left to right')\n",
    "    train_ppl += [train_log['ppl']]\n",
    "\n",
    "    if VERBOSE: \n",
    "        for key, value in train_log.items():\n",
    "            print_scalar('train/%s' % key, value, writes)\n",
    "        print('')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_log  = full_epoch(epoch, 'valid')\n",
    "        valid_ppl += [valid_log['ppl']]\n",
    "\n",
    "        if VERBOSE: \n",
    "            for key, value in valid_log.items():\n",
    "                print_scalar('valid/%s' % key, value, writes)\n",
    "            print('')\n",
    "        \n",
    "    writes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(train_ppl, valid_ppl, 'BASELINE (left-to-right ordering)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, max_len, start_symbol=2, take_max=True):\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).long().cuda()\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(\n",
    "                           ys,\n",
    "                           (subsequent_mask(ys.size(1)).type_as(ys)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        if take_max:\n",
    "            _, next_word = torch.max(prob, dim = 1)\n",
    "        else:\n",
    "            dist = torch.distributions.Categorical(prob.exp())\n",
    "            next_word = dist.sample()\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).long().fill_(next_word).cuda()], dim=1)\n",
    "        ys = ys.cuda()\n",
    "    return ys\n",
    "\n",
    "def low_entropy_decoding(model, max_len, sos_token, pad_token):\n",
    "    import pdb; pdb.set_trace()\n",
    "    ys = torch.ones(1, max_len).fill_(pad_token).long().cuda()\n",
    "    ys[0, 0] = sos_token\n",
    "\n",
    "    mask = torch.zeros(1, max_len, max_len).byte().cuda()\n",
    "    \n",
    "    # all tokens can look at themselves\n",
    "    # mask += torch.eye(max_len).unsqueeze(0).byte().cuda()  \n",
    "    \n",
    "    # TODO remove this\n",
    "    # mask = make_std_mask(torch.zeros_like(ys), pad_token)\n",
    "    # mask = mask - torch.eye(mask.size(-1)).unsqueeze(0).byte().cuda()\n",
    "    # test = subsequent_mask(ys.size(1))\n",
    "    \n",
    "    # all tokens can look at the sos token\n",
    "    mask[:, :, 0] = 1\n",
    "  \n",
    "\n",
    "    for t in range(max_len):\n",
    "        out  = model.decode(ys, mask)\n",
    "        prob = model.generator(out)\n",
    "        dist = torch.distributions.Categorical(prob.squeeze().exp())\n",
    "        \n",
    "        # zero-out words that have already been generated\n",
    "        # TODO\n",
    "        position = dist.entropy().argmin()\n",
    "        \n",
    "        # update the mask to take into account this new word\n",
    "        # TODO\n",
    "        sample = dist.sample()[position]\n",
    "    \n",
    "        \n",
    "def to_readable(vocab, matrix):\n",
    "    if isinstance(vocab, torchtext.data.field.Field):\n",
    "        vocab = vocab.vocab.itos\n",
    "\n",
    "    sentences = []\n",
    "    for line in matrix:\n",
    "        sentence = ''\n",
    "        for token in line:\n",
    "            sentence += vocab[token] + ' '\n",
    "        sentence = sentence.replace('<pad>', '')\n",
    "        sentences += [sentence]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2, 1567, 4251, 4859, 2434, 2952,  950, 4538, 3310, 3730,  118,  573,\n",
      "          647, 3112, 1803, 4194,  181, 1182, 5018, 4367, 3787, 1343, 3186, 4486,\n",
      "         1041,  358, 2644, 4258, 1991, 3548, 4625,  943,  363,  622, 3036, 5023,\n",
      "         1924, 3680, 4832, 4861, 4643, 1275, 3487, 4963,  513, 4079, 4469,  836,\n",
      "          206, 1648,  274]], device='cuda:0')\n",
      "['the hotel boom 20th convicted organisations isis courses bars emerging take 9 stage pursue defend racial end charge romantic heritage closure network funded satellite august ever minds landed ongoing deployed simpson increased report outside prosecutor brian buying hanging bennett adam blocks daily lay orange living passes tourist girl man bringing keep ']\n"
     ]
    }
   ],
   "source": [
    "out = greedy_decode(gen, 51, take_max=False)\n",
    "print(out)\n",
    "print(to_readable(input_field, out))\n",
    "# out = low_entropy_decoding(gen, 51, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp2 : Proposed Model (random ordering masking for training and eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/nll                                @ write 0 = 1.6476\n",
      "train/ppl                                @ write 0 = 13.7395\n",
      "\n",
      "valid/nll                                @ write 0 = 2.2066\n",
      "valid/ppl                                @ write 0 = 9.1447\n",
      "\n",
      "train/nll                                @ write 1 = 1.2710\n",
      "train/ppl                                @ write 1 = 3.5674\n",
      "\n",
      "valid/nll                                @ write 1 = 2.0846\n",
      "valid/ppl                                @ write 1 = 8.0797\n",
      "\n",
      "train/nll                                @ write 2 = 1.2214\n",
      "train/ppl                                @ write 2 = 3.3946\n",
      "\n",
      "valid/nll                                @ write 2 = 2.0463\n",
      "valid/ppl                                @ write 2 = 7.7770\n",
      "\n",
      "train/nll                                @ write 3 = 1.1943\n",
      "train/ppl                                @ write 3 = 3.3035\n",
      "\n",
      "valid/nll                                @ write 3 = 2.0221\n",
      "valid/ppl                                @ write 3 = 7.5993\n",
      "\n",
      "train/nll                                @ write 4 = 1.1777\n",
      "train/ppl                                @ write 4 = 3.2493\n",
      "\n",
      "valid/nll                                @ write 4 = 2.0042\n",
      "valid/ppl                                @ write 4 = 7.4551\n",
      "\n",
      "train/nll                                @ write 5 = 1.1632\n",
      "train/ppl                                @ write 5 = 3.2022\n",
      "\n",
      "valid/nll                                @ write 5 = 1.9831\n",
      "valid/ppl                                @ write 5 = 7.3017\n",
      "\n",
      "train/nll                                @ write 6 = 1.1525\n",
      "train/ppl                                @ write 6 = 3.1683\n",
      "\n",
      "valid/nll                                @ write 6 = 1.9686\n",
      "valid/ppl                                @ write 6 = 7.1992\n",
      "\n",
      "train/nll                                @ write 7 = 1.1442\n",
      "train/ppl                                @ write 7 = 3.1419\n",
      "\n",
      "valid/nll                                @ write 7 = 1.9593\n",
      "valid/ppl                                @ write 7 = 7.1292\n",
      "\n",
      "train/nll                                @ write 8 = 1.1338\n",
      "train/ppl                                @ write 8 = 3.1095\n",
      "\n",
      "valid/nll                                @ write 8 = 1.9510\n",
      "valid/ppl                                @ write 8 = 7.0721\n",
      "\n",
      "train/nll                                @ write 9 = 1.1278\n",
      "train/ppl                                @ write 9 = 3.0908\n",
      "\n",
      "valid/nll                                @ write 9 = 1.9468\n",
      "valid/ppl                                @ write 9 = 7.0456\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-676314ccbfca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_log\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mfull_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'random'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_ppl\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ppl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4608112a6b4d>\u001b[0m in \u001b[0;36mfull_epoch\u001b[0;34m(epoch_no, split, mask_type)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m              \u001b[0moptimizer_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m              \u001b[0mrecon_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m              \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m              \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch_py3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch_py3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "VERBOSE = True\n",
    "gen  = make_model(len(input_field.vocab.itos), N=2, h=4).cuda()\n",
    "optimizer_gen = torch.optim.Adam(gen.parameters())\n",
    "\n",
    "train_ppl, valid_ppl, writes = [], [], 0\n",
    "\n",
    "for epoch in range(30):\n",
    "    train_log  = full_epoch(epoch, 'train', mask_type = 'random')\n",
    "    train_ppl += [train_log['ppl']]\n",
    "\n",
    "    if VERBOSE: \n",
    "        for key, value in train_log.items():\n",
    "            print_scalar('train/%s' % key, value, writes)\n",
    "        print('')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_log  = full_epoch(epoch, 'valid', mask_type='random')\n",
    "        valid_ppl += [valid_log['ppl']]\n",
    "\n",
    "        if VERBOSE: \n",
    "            for key, value in valid_log.items():\n",
    "                print_scalar('valid/%s' % key, value, writes)\n",
    "            print('')\n",
    "        \n",
    "    writes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_ppl[2:], valid_ppl[2:], 'LM-MADE (random train/test masks)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = greedy_decode(gen, 51, take_max=False)\n",
    "print(to_readable(input_field, out))\n",
    "out = low_entropy_decoding(gen, 51, 0, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity wise, these results are very promising. Let's see if this gain simply comes from evaluating with random orderings, or that training with random masks actually helps. To do so, we train using the regular ordering, and evaluate with random masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "gen  = make_model(10000 + 1, N=2, h=4).cuda()\n",
    "optimizer_gen = torch.optim.Adam(gen.parameters())\n",
    "\n",
    "train_ppl, valid_ppl, writes = [], [], 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_log  = full_epoch(epoch, 'train', mask_type = 'left to right')\n",
    "    train_ppl += [train_log['ppl']]\n",
    "\n",
    "    if VERBOSE: \n",
    "        for key, value in train_log.items():\n",
    "            print_scalar('train/%s' % key, value, writes)\n",
    "        print('')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_log  = full_epoch(epoch, 'valid', mask_type='random')\n",
    "        valid_ppl += [valid_log['ppl']]\n",
    "\n",
    "        if VERBOSE: \n",
    "            for key, value in valid_log.items():\n",
    "                print_scalar('valid/%s' % key, value, writes)\n",
    "            print('')\n",
    "        \n",
    "    writes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_ppl, valid_ppl, 'regular training, random mask for testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing to try : Train on random orderings but evaluate only on left-to-right orderings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "gen  = make_model(10000 + 1, N=2, h=4).cuda()\n",
    "optimizer_gen = torch.optim.Adam(gen.parameters())\n",
    "\n",
    "train_ppl, valid_ppl, writes = [], [], 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_log  = full_epoch(epoch, 'train', mask_type = 'random')\n",
    "    train_ppl += [train_log['ppl']]\n",
    "\n",
    "    if VERBOSE: \n",
    "        for key, value in train_log.items():\n",
    "            print_scalar('train/%s' % key, value, writes)\n",
    "        print('')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_log  = full_epoch(epoch, 'valid', mask_type='left to right')\n",
    "        valid_ppl += [valid_log['ppl']]\n",
    "\n",
    "        if VERBOSE: \n",
    "            for key, value in valid_log.items():\n",
    "                print_scalar('valid/%s' % key, value, writes)\n",
    "            print('')\n",
    "        \n",
    "    writes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_ppl, valid_ppl, 'random training, left-to-right testing')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_py3]",
   "language": "python",
   "name": "conda-env-pytorch_py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
