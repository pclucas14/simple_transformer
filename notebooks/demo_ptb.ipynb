{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Masking (MADE style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml/lpagec/anaconda2/envs/pytorch_py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ml/lpagec/anaconda2/envs/pytorch_py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext import data, datasets\n",
    "from collections import OrderedDict as OD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../')\n",
    "from transformer import * \n",
    "from utils       import * \n",
    "from custom_ds   import *\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)  \n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "VERBOSE = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "input_field, train_iter, val_iter, test_iter = load_data(path='../data/ptb', device=0, repeat=False)\n",
    "iterators = {'train': train_iter, 'valid': val_iter, 'test': test_iter}\n",
    "VOCAB_SIZE = len(input_field.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params 16556817\n"
     ]
    }
   ],
   "source": [
    "# create model and ship to GPU\n",
    "gen  = make_model(VOCAB_SIZE, N=2, h=4).cuda()\n",
    "# print(gen)\n",
    "print('number of params', sum([np.prod([int(y) for y in x.shape]) for x in gen.parameters()]))\n",
    "\n",
    "# build optimizer\n",
    "optimizer_gen = torch.optim.Adam(gen.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over the whole dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot perplexity graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(train_ppl, valid_ppl, title):\n",
    "    train, valid = [], []\n",
    "    for tt, vv in zip(train_ppl, valid_ppl):\n",
    "        train += [torch.stack(tt).mean().item()]\n",
    "        valid += [torch.stack(vv).mean().item()]\n",
    "        \n",
    "    plt.scatter(np.arange(len(train)), train, label='train ppl')\n",
    "    plt.scatter(np.arange(len(valid)), valid, label='valid ppl')\n",
    "    plt.legend()\n",
    "    plt.hlines(min(valid), 0, len(valid), linestyles='dashed')\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlim(-0.1, len(valid))\n",
    "    plt.yticks([min(valid)] + [x for x in np.linspace(0, max(train + valid), 5)][1:])\n",
    "    plt.title(title)\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\ndef build_ar_masks(lens, order='random', num_swaps=None):\\n    assert order in ['random', 'left to right']\\n    max_len = max(lens)\\n    masks, orders, targets, distances = [], [], [], []\\n    for len_ in lens:\\n        arr = np.arange(len_)\\n        \\n        if order == 'random':\\n            if num_swaps is None:\\n                np.random.shuffle(arr)\\n                distances += [np.absolute(np.arange(len_) - arr).sum()]\\n            else:\\n                for _ in range(num_swaps):\\n                    s_ind = np.random.randint(len_)\\n                    t_ind = np.random.randint(len_)\\n                    temp   = arr[s_ind]\\n                    arr[s_ind] = arr[t_ind]\\n                    arr[t_ind] = temp\\n                    distances += [num_swaps]                 \\n        else:\\n            distances += [0]\\n\\n        arr = np.concatenate([arr, np.arange(len_, max_len)])\\n\\n        orders += [arr]\\n\\n        rev = [arr[arr[j]] for j in range(len_)]\\n        target = []\\n        mask = np.zeros((max_len, max_len))\\n        for j, row in enumerate(mask[:len_]):\\n            # row[i] = 1\\n            # find index with i\\n            index = np.where(arr == j)[0][0]\\n            row[arr[:index]] = 1\\n\\n            if index < len_ - 1: # not last\\n                target += [arr[index+1]]\\n            else:\\n                target += [-1]\\n\\n        target += [-1] * (max_len - len(target))\\n        mask = mask + np.eye(mask.shape[0])\\n        mask[len_:, len_:] = 0\\n\\n        masks += [mask]\\n        targets += [target]\\n\\n    return np.stack(masks), np.stack(orders), np.stack(targets), np.array(distances)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "def build_ar_masks(lens, order='random', num_swaps=None):\n",
    "    assert order in ['random', 'left to right']\n",
    "    max_len = max(lens)\n",
    "    masks, orders, targets, distances = [], [], [], []\n",
    "    for len_ in lens:\n",
    "        arr = np.arange(len_)\n",
    "        \n",
    "        if order == 'random':\n",
    "            if num_swaps is None:\n",
    "                np.random.shuffle(arr)\n",
    "                distances += [np.absolute(np.arange(len_) - arr).sum()]\n",
    "            else:\n",
    "                for _ in range(num_swaps):\n",
    "                    s_ind = np.random.randint(len_)\n",
    "                    t_ind = np.random.randint(len_)\n",
    "                    temp   = arr[s_ind]\n",
    "                    arr[s_ind] = arr[t_ind]\n",
    "                    arr[t_ind] = temp\n",
    "                    distances += [num_swaps]                 \n",
    "        else:\n",
    "            distances += [0]\n",
    "\n",
    "        arr = np.concatenate([arr, np.arange(len_, max_len)])\n",
    "\n",
    "        orders += [arr]\n",
    "\n",
    "        rev = [arr[arr[j]] for j in range(len_)]\n",
    "        target = []\n",
    "        mask = np.zeros((max_len, max_len))\n",
    "        for j, row in enumerate(mask[:len_]):\n",
    "            # row[i] = 1\n",
    "            # find index with i\n",
    "            index = np.where(arr == j)[0][0]\n",
    "            row[arr[:index]] = 1\n",
    "\n",
    "            if index < len_ - 1: # not last\n",
    "                target += [arr[index+1]]\n",
    "            else:\n",
    "                target += [-1]\n",
    "\n",
    "        target += [-1] * (max_len - len(target))\n",
    "        mask = mask + np.eye(mask.shape[0])\n",
    "        mask[len_:, len_:] = 0\n",
    "\n",
    "        masks += [mask]\n",
    "        targets += [target]\n",
    "\n",
    "    return np.stack(masks), np.stack(orders), np.stack(targets), np.array(distances)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_epoch(epoch_no, split, mask_type='left to right'):\n",
    "    loader = iterators[split]\n",
    "\n",
    "    # create logging containers\n",
    "    logs = OD()\n",
    "    for name in ['nll', 'ppl']:\n",
    "        logs[name] = []\n",
    "\n",
    "    gen.train() if split == 'train' else gen.eval()\n",
    "\n",
    "    # Training loop\n",
    "    for i, minibatch in enumerate(loader):\n",
    "        \n",
    "        text = minibatch.text.cuda()\n",
    "        input = text\n",
    "        \n",
    "        lens = (text != 1).sum(dim=1).byte().cpu().data.numpy()\n",
    "        masks, orders, tt, dists = [torch.from_numpy(x).long().cuda() for x in build_ar_masks(lens, order=mask_type)]\n",
    "\n",
    "        if i == 0 : \n",
    "            print(orders[0])\n",
    "        \n",
    "        offset = (torch.arange(orders.size(0)) * (orders.size(1))).unsqueeze(1).long().cuda()\n",
    "        new_order = torch.take(input,  offset + tt)\n",
    "        target = new_order\n",
    "\n",
    "        # one token per sentence does not have a target anymore, here is the mask to mask it out\n",
    "        mask_eos = tt != -1  # -1 was used to denote the target of the last token\n",
    "        mask_pad = text != 1 #  1 is <PAD> token\n",
    "        mask = torch.min(mask_eos, mask_pad)\n",
    "\n",
    "        tt = tt.clamp(min=0)\n",
    "            \n",
    "        bs, seq_len = input.size()\n",
    "        # we pass in the target so that the model can see the positional embedding of the word it needs to predict\n",
    "        logits = gen(input, masks, target_pos=tt)\n",
    "        \n",
    "        recon_loss = F.cross_entropy(logits.view(bs * seq_len, -1), target.flatten(), reduction='none')\n",
    "        recon_loss = recon_loss.reshape(*input.size())\n",
    "        \n",
    "        # mask out the conditionals with no target and mask out the pad tokens\n",
    "        recon_loss = recon_loss * mask.float()\n",
    "        recon_loss = recon_loss.sum() / mask.sum().float()\n",
    "\n",
    "        if gen.training:\n",
    "             optimizer_gen.zero_grad()\n",
    "             recon_loss.backward()\n",
    "             params = optimizer_gen.param_groups[0]['params']\n",
    "             torch.nn.utils.clip_grad_norm_(params, 10, norm_type=2)\n",
    "             optimizer_gen.step()\n",
    "         \n",
    "        logs['nll']  += [recon_loss.data]\n",
    "        logs['ppl']  += [recon_loss.exp().data]\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp2 : Proposed Model (random ordering masking for training and eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19, 22,  4,  8, 11, 14,  6, 12,  7, 10, 13, 20,  9,  2,  3, 21,  0, 18,\n",
      "        17,  5,  1, 15, 16, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54], device='cuda:0')\n",
      "train/nll                                @ write 0 = 5.2252\n",
      "train/ppl                                @ write 0 = 186.2739\n",
      "\n",
      "tensor([ 5,  0,  1,  8,  6, 10,  4,  7,  2,  3, 11,  9], device='cuda:0')\n",
      "valid/nll                                @ write 0 = 5.1553\n",
      "\n",
      "valid/ppl                                @ write 0 = 175.4467\n",
      "\n",
      "tensor([ 0,  4,  6,  7,  2,  5,  9,  3,  8,  1, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68],\n",
      "       device='cuda:0')\n",
      "train/nll                                @ write 1 = 5.0519\n",
      "train/ppl                                @ write 1 = 156.6082\n",
      "\n",
      "tensor([ 1,  4,  8,  7,  0,  6,  2,  3,  5, 10,  9, 11], device='cuda:0')\n",
      "valid/nll                                @ write 1 = 5.0544\n",
      "\n",
      "valid/ppl                                @ write 1 = 158.1853\n",
      "\n",
      "tensor([27,  4, 18,  2, 22, 14, 17, 30, 32, 26,  5, 36,  0, 23, 31, 28,  3,  1,\n",
      "        34, 13, 11, 29, 37, 24, 12, 10, 15, 21,  7, 38,  9, 16, 35, 19, 39, 25,\n",
      "        33,  6,  8, 20, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55], device='cuda:0')\n",
      "train/nll                                @ write 2 = 4.9206\n",
      "train/ppl                                @ write 2 = 137.2913\n",
      "\n",
      "tensor([ 6,  2,  8,  3,  1,  0,  5,  9,  4, 10,  7, 11], device='cuda:0')\n",
      "valid/nll                                @ write 2 = 4.9899\n",
      "\n",
      "valid/ppl                                @ write 2 = 148.5220\n",
      "\n",
      "tensor([ 1, 10,  3, 19, 15,  4, 23, 14, 20,  2, 27, 25,  5, 26, 16, 24,  9, 22,\n",
      "        17, 29,  8, 12, 11,  0,  7,  6, 28, 18, 13, 21, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5343ab201cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_log\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mfull_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'random'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_ppl\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ppl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-324f273e9695>\u001b[0m in \u001b[0;36mfull_epoch\u001b[0;34m(epoch_no, split, mask_type)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m              \u001b[0moptimizer_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m              \u001b[0mrecon_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m              \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m              \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch_py3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch_py3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "VERBOSE = True\n",
    "#gen  = make_model(len(input_field.vocab.itos), N=2, h=4).cuda()\n",
    "#optimizer_gen = torch.optim.Adam(gen.parameters())\n",
    "\n",
    "train_ppl, valid_ppl, writes = [], [], 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_log  = full_epoch(epoch, 'train', mask_type = 'random')\n",
    "    train_ppl += [train_log['ppl']]\n",
    "\n",
    "    if VERBOSE: \n",
    "        for key, value in train_log.items():\n",
    "            print_scalar('train/%s' % key, value, writes)\n",
    "        print('')\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        valid_log  = full_epoch(epoch, 'valid', mask_type='random')\n",
    "        valid_ppl += [valid_log['ppl']]\n",
    "\n",
    "        if VERBOSE: \n",
    "            for key, value in valid_log.items():\n",
    "                print_scalar('valid/%s' % key, value, writes)\n",
    "                print('')\n",
    "        \n",
    "    writes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_ppl, valid_ppl, 'BASELINE (left-to-right ordering)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, max_len, start_symbol=2, take_max=True):\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).long().cuda()\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(\n",
    "                           ys,\n",
    "                           (subsequent_mask(ys.size(1)).type_as(ys)), target_pos=None)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        if take_max:\n",
    "            _, next_word = torch.max(prob, dim = 1)\n",
    "        else:\n",
    "            dist = torch.distributions.Categorical(prob.exp())\n",
    "            next_word = dist.sample()\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).long().fill_(next_word).cuda()], dim=1)\n",
    "        ys = ys.cuda()\n",
    "    return ys\n",
    "\n",
    "def low_entropy_decoding(model, max_len, sos_token, pad_token):\n",
    "    ys = torch.ones(1, max_len).fill_(pad_token).long().cuda()\n",
    "    ys[0, 0] = sos_token\n",
    "\n",
    "    mask = torch.zeros(1, max_len, max_len).byte().cuda()\n",
    "    \n",
    "    # all tokens can look at the sos token\n",
    "    mask[:, :, 0] = 1\n",
    "  \n",
    "    target_pos = torch.arange(max_len).unsqueeze(0).long().cuda()\n",
    "\n",
    "    for t in range(max_len):\n",
    "        out  = model.decode(ys, mask, target_pos)\n",
    "        prob = model.generator(out)\n",
    "        dist = torch.distributions.Categorical(prob.squeeze().exp())\n",
    "\n",
    "        entropies = dist.entropy()                \n",
    "        # zero-out words that have already been generated\n",
    "        mask_t = (ys != pad_token).squeeze()\n",
    "        entropies.masked_fill_(mask_t, 999999)\n",
    "\n",
    "        position = entropies.argmin()\n",
    "        sample = dist.sample()[position]\n",
    "\n",
    "        # update the mask to take into account this new word\n",
    "        mask[:, :, position] = 1\n",
    "        ys[:, position] = sample\n",
    "        \n",
    "    return ys\n",
    "    \n",
    "        \n",
    "def to_readable(vocab, matrix):\n",
    "    if isinstance(vocab, torchtext.data.field.Field):\n",
    "        vocab = vocab.vocab.itos\n",
    "\n",
    "    sentences = []\n",
    "    for line in matrix:\n",
    "        sentence = ''\n",
    "        for token in line:\n",
    "            sentence += vocab[token] + ' '\n",
    "        sentence = sentence.replace('<pad>', '')\n",
    "        sentences += [sentence]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = greedy_decode(gen, 51, take_max=False)\n",
    "print(out)\n",
    "print(to_readable(input_field, out))\n",
    "out = low_entropy_decoding(gen, 51, 0, 1)\n",
    "print(to_readable(input_field , out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp1 : Baseline Model (left to right masking for training and eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = True\n",
    "gen  = make_model(len(input_field.vocab.itos), N=2, h=4).cuda()\n",
    "optimizer_gen = torch.optim.Adam(gen.parameters())\n",
    "\n",
    "train_ppl, valid_ppl, writes = [], [], 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_log  = full_epoch(epoch, 'train', mask_type = 'left to right')\n",
    "    train_ppl += [train_log['ppl']]\n",
    "\n",
    "    if VERBOSE: \n",
    "        for key, value in train_log.items():\n",
    "            print_scalar('train/%s' % key, value, writes)\n",
    "        print('')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_log  = full_epoch(epoch, 'valid')\n",
    "        valid_ppl += [valid_log['ppl']]\n",
    "\n",
    "        if VERBOSE: \n",
    "            for key, value in valid_log.items():\n",
    "                print_scalar('valid/%s' % key, value, writes)\n",
    "            print('')\n",
    "        \n",
    "    writes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_ppl[2:], valid_ppl[2:], 'LM-MADE (random train/test masks)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = greedy_decode(gen, 51, take_max=False)\n",
    "print(to_readable(input_field, out))\n",
    "out = low_entropy_decoding(gen, 51, 0, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity wise, these results are very promising. Let's see if this gain simply comes from evaluating with random orderings, or that training with random masks actually helps. To do so, we train using the regular ordering, and evaluate with random masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "gen  = make_model(10000 + 1, N=2, h=4).cuda()\n",
    "optimizer_gen = torch.optim.Adam(gen.parameters())\n",
    "\n",
    "train_ppl, valid_ppl, writes = [], [], 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_log  = full_epoch(epoch, 'train', mask_type = 'left to right')\n",
    "    train_ppl += [train_log['ppl']]\n",
    "\n",
    "    if VERBOSE: \n",
    "        for key, value in train_log.items():\n",
    "            print_scalar('train/%s' % key, value, writes)\n",
    "        print('')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_log  = full_epoch(epoch, 'valid', mask_type='random')\n",
    "        valid_ppl += [valid_log['ppl']]\n",
    "\n",
    "        if VERBOSE: \n",
    "            for key, value in valid_log.items():\n",
    "                print_scalar('valid/%s' % key, value, writes)\n",
    "            print('')\n",
    "        \n",
    "    writes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_ppl, valid_ppl, 'regular training, random mask for testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing to try : Train on random orderings but evaluate only on left-to-right orderings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "gen  = make_model(10000 + 1, N=2, h=4).cuda()\n",
    "optimizer_gen = torch.optim.Adam(gen.parameters())\n",
    "\n",
    "train_ppl, valid_ppl, writes = [], [], 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_log  = full_epoch(epoch, 'train', mask_type = 'random')\n",
    "    train_ppl += [train_log['ppl']]\n",
    "\n",
    "    if VERBOSE: \n",
    "        for key, value in train_log.items():\n",
    "            print_scalar('train/%s' % key, value, writes)\n",
    "        print('')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_log  = full_epoch(epoch, 'valid', mask_type='left to right')\n",
    "        valid_ppl += [valid_log['ppl']]\n",
    "\n",
    "        if VERBOSE: \n",
    "            for key, value in valid_log.items():\n",
    "                print_scalar('valid/%s' % key, value, writes)\n",
    "            print('')\n",
    "        \n",
    "    writes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_ppl, valid_ppl, 'random training, left-to-right testing')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_py3]",
   "language": "python",
   "name": "conda-env-pytorch_py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
